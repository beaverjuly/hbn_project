{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f768d32d-a181-4db5-a36e-5a9487d5b9f3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53e61bb4-114a-484b-83c5-0ecebfd04570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, io, re, csv, json, time, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Local folders (kept out of Git if \".gitignore\" includes \"data/\")\n",
    "DATA = Path(\"data\"); RAW = DATA/\"raw\"; INTERIM = DATA/\"interim\"\n",
    "RAW.mkdir(parents=True, exist_ok=True); INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base URL (use HTTP, not HTTPS, to avoid host certificate mismatch on this server)\n",
    "BASE = \"http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/\"\n",
    "\n",
    "# Pheno releases you want to pull\n",
    "PHENO_FILES = [\n",
    "    \"HBN_R1_1_Pheno.csv\", \"HBN_R2_1_Pheno.csv\", \"HBN_R3_Pheno.csv\", \"HBN_R4_Pheno.csv\",\n",
    "    \"HBN_R5_Pheno.csv\",   \"HBN_R6_Pheno.csv\",   \"HBN_R7_Pheno.csv\", \"HBN_R8_Pheno.csv\",\n",
    "    \"HBN_R9_Pheno.csv\",   \"HBN_R10_Pheno.csv\",  \"HBN_R11_Pheno.csv\"\n",
    "]\n",
    "\n",
    "DIAG_FILE = \"Diagnosis_ClinicianConsensus.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2a98d-4563-45b4-bf21-4045c82eb17e",
   "metadata": {},
   "source": [
    "# HTTP fetch helper + caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7b7be6c-66ed-4cf5-8412-fc935a5122d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_text(url: str, timeout: int = 60) -> str:\n",
    "    \"\"\"GET text from URL (force http on this host).\"\"\"\n",
    "    if url.startswith(\"https://fcon_1000.projects.nitrc.org\"):\n",
    "        url = url.replace(\"https://\", \"http://\", 1)\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def read_table_smart(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Download a delimited text file and read with a sniffed separator.\"\"\"\n",
    "    text = http_text(url)\n",
    "    sample = text[:5000]\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"\\t\",\"|\"])\n",
    "        sep = dialect.delimiter\n",
    "    except Exception:\n",
    "        sep = max([\",\",\";\",\"\\t\",\"|\"], key=sample.count)\n",
    "    df = pd.read_csv(io.StringIO(text), sep=sep, engine=\"python\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def save_raw(df: pd.DataFrame, name: str):\n",
    "    path = RAW / name\n",
    "    df.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "def norm_eid(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().upper()\n",
    "    s = re.sub(r\"[^A-Z0-9]\", \"\", s)  # remove spaces/dashes\n",
    "    return s or np.nan\n",
    "\n",
    "def release_rank_from_filename(name: str) -> float:\n",
    "    \"\"\"\n",
    "    'HBN_R1_1_Pheno.csv' -> 1.1 , 'HBN_R10_Pheno.csv' -> 10.0, 'HBN_R11_Pheno.csv' -> 11.0\n",
    "    \"\"\"\n",
    "    m = re.search(r\"_R(\\d+)(?:_(\\d+))?_Pheno\\.csv$\", name)\n",
    "    if not m:\n",
    "        return 0.0\n",
    "    major = int(m.group(1))\n",
    "    minor = int(m.group(2)) if m.group(2) else 0\n",
    "    return float(f\"{major}.{minor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "737c6b74-ea3a-4027-94b0-b5908bf0a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HBN_R1_1_Pheno.csv: (797, 9)\n",
      "Loaded HBN_R2_1_Pheno.csv: (256, 9)\n",
      "Loaded HBN_R3_Pheno.csv: (317, 9)\n",
      "Loaded HBN_R4_Pheno.csv: (558, 9)\n",
      "Loaded HBN_R5_Pheno.csv: (391, 9)\n",
      "Loaded HBN_R6_Pheno.csv: (336, 9)\n",
      "Loaded HBN_R7_Pheno.csv: (692, 9)\n",
      "Loaded HBN_R8_Pheno.csv: (470, 9)\n",
      "Loaded HBN_R9_Pheno.csv: (422, 9)\n",
      "Loaded HBN_R10_Pheno.csv: (847, 9)\n",
      "Loaded HBN_R11_Pheno.csv: (1160, 9)\n",
      "pheno_all: (6246, 10)\n",
      "pheno_latest (one row per EID): (3432, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/raw/HBN_pheno_latest.csv')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pheno_frames = []\n",
    "for fname in PHENO_FILES:\n",
    "    url = BASE + fname\n",
    "    try:\n",
    "        df = read_table_smart(url)\n",
    "        # cache the exact download\n",
    "        save_raw(df, fname)\n",
    "        # add bookkeeping columns\n",
    "        df[\"_release_file\"] = fname\n",
    "        df[\"_release_rank\"] = release_rank_from_filename(fname)\n",
    "        # normalize EID (prefer explicit 'EID' column, else try to infer)\n",
    "        if \"EID\" in df.columns:\n",
    "            df[\"_EID\"] = df[\"EID\"].map(norm_eid)\n",
    "        else:\n",
    "            # try to guess any ID-like column\n",
    "            idcol = None\n",
    "            for c in df.columns:\n",
    "                if re.fullmatch(r\"(participant_)?eid\", c, flags=re.I):\n",
    "                    idcol = c; break\n",
    "            if idcol:\n",
    "                df[\"_EID\"] = df[idcol].map(norm_eid)\n",
    "            else:\n",
    "                df[\"_EID\"] = np.nan\n",
    "        pheno_frames.append(df)\n",
    "        print(f\"Loaded {fname}: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: failed to load {fname}: {e}\")\n",
    "\n",
    "# Concatenate all releases\n",
    "pheno_all = pd.concat(pheno_frames, ignore_index=True)\n",
    "print(\"pheno_all:\", pheno_all.shape)\n",
    "\n",
    "# Keep the **latest** row per participant (by _release_rank)\n",
    "pheno_all = pheno_all[pheno_all[\"_EID\"].notna()]\n",
    "pheno_latest = (pheno_all.sort_values([\"_EID\",\"_release_rank\"])\n",
    "                          .drop_duplicates(\"_EID\", keep=\"last\"))\n",
    "print(\"pheno_latest (one row per EID):\", pheno_latest.shape)\n",
    "\n",
    "# Save concatenated & latest\n",
    "save_raw(pheno_all,   \"HBN_pheno_all_concat.csv\")\n",
    "save_raw(pheno_latest.drop(columns=[\"_release_file\",\"_release_rank\"]), \"HBN_pheno_latest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145692f-f72b-4e9c-85d8-b30ccfd4a93d",
   "metadata": {},
   "source": [
    "## Download & parse Diagnosis_ClinicianConsensus.csv, extract EID from “Identifiers”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72c74f3f-5d5a-4991-94d3-c3b54a5ad262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis rows with resolvable EID: 2221 of 2569\n"
     ]
    }
   ],
   "source": [
    "# Raw diagnosis table\n",
    "diag = read_table_smart(BASE + DIAG_FILE)\n",
    "save_raw(diag, DIAG_FILE)\n",
    "\n",
    "# Build a set of known EIDs from pheno_latest to help disambiguate tokens\n",
    "eid_set = set(pheno_latest[\"_EID\"].dropna().unique())\n",
    "\n",
    "# Extract EID candidates from \"Identifiers\"\n",
    "if \"Identifiers\" not in diag.columns:\n",
    "    raise RuntimeError(\"Diagnosis table has no 'Identifiers' column. Inspect columns: %s\" % diag.columns.tolist())\n",
    "\n",
    "def extract_eid_from_identifiers(val, candidates):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).upper()\n",
    "    # split on common delimiters and whitespace\n",
    "    tokens = re.split(r\"[;,\\|\\s]+\", s)\n",
    "    # first try exact matches to known EIDs\n",
    "    for t in tokens:\n",
    "        tnorm = norm_eid(t)\n",
    "        if tnorm in candidates:\n",
    "            return tnorm\n",
    "    # fallback regex for HBN-like tokens\n",
    "    m = re.search(r\"\\bHBN[A-Z0-9]+\\b\", s)\n",
    "    if m:\n",
    "        tnorm = norm_eid(m.group(0))\n",
    "        if tnorm in candidates:\n",
    "            return tnorm\n",
    "    return np.nan\n",
    "\n",
    "diag = diag.copy()\n",
    "diag[\"_EID\"] = diag[\"Identifiers\"].apply(lambda v: extract_eid_from_identifiers(v, eid_set))\n",
    "diag_keyed = diag.dropna(subset=[\"_EID\"]).drop_duplicates(\"_EID\")\n",
    "print(\"Diagnosis rows with resolvable EID:\", diag_keyed.shape[0], \"of\", diag.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0488466-8ee7-483f-9dbb-d84cfd6d5eea",
   "metadata": {},
   "source": [
    "# Merge latest pheno with diagnosis (inner join on EID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ff2695f-7c51-406c-b2e8-0dd3d34c3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (2221, 174)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_EID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01_ByHx</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01_Cat</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01_Code</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01_Confirmed</th>\n",
       "      <th>Diagnosis_ClinicianConsensus,DX_01_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARAA075AMK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.728040</td>\n",
       "      <td>No Diagnosis Given</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Diagnosis Given</td>\n",
       "      <td>No Diagnosis Given</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARAA112DMH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.545744</td>\n",
       "      <td>ADHD-Combined Type</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neurodevelopmental Disorders</td>\n",
       "      <td>F90.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARAA117NEJ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.475929</td>\n",
       "      <td>ADHD-Combined Type</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neurodevelopmental Disorders</td>\n",
       "      <td>F90.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARAA536PTU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.998402</td>\n",
       "      <td>ADHD-Inattentive Type</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neurodevelopmental Disorders</td>\n",
       "      <td>F90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARAA948VFH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.982660</td>\n",
       "      <td>ADHD-Combined Type</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neurodevelopmental Disorders</td>\n",
       "      <td>F90.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           _EID  Sex        Age Diagnosis_ClinicianConsensus,DX_01  \\\n",
       "0  NDARAA075AMK  1.0   6.728040                 No Diagnosis Given   \n",
       "1  NDARAA112DMH  0.0   5.545744                 ADHD-Combined Type   \n",
       "2  NDARAA117NEJ  0.0   7.475929                 ADHD-Combined Type   \n",
       "3  NDARAA536PTU  0.0  11.998402              ADHD-Inattentive Type   \n",
       "4  NDARAA948VFH  1.0   7.982660                 ADHD-Combined Type   \n",
       "\n",
       "   Diagnosis_ClinicianConsensus,DX_01_ByHx  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "\n",
       "  Diagnosis_ClinicianConsensus,DX_01_Cat  \\\n",
       "0                     No Diagnosis Given   \n",
       "1           Neurodevelopmental Disorders   \n",
       "2           Neurodevelopmental Disorders   \n",
       "3           Neurodevelopmental Disorders   \n",
       "4           Neurodevelopmental Disorders   \n",
       "\n",
       "  Diagnosis_ClinicianConsensus,DX_01_Code  \\\n",
       "0                      No Diagnosis Given   \n",
       "1                                   F90.2   \n",
       "2                                   F90.2   \n",
       "3                                   F90.0   \n",
       "4                                   F90.2   \n",
       "\n",
       "   Diagnosis_ClinicianConsensus,DX_01_Confirmed  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "\n",
       "   Diagnosis_ClinicianConsensus,DX_01_New  \n",
       "0                                     0.0  \n",
       "1                                     0.0  \n",
       "2                                     1.0  \n",
       "3                                     0.0  \n",
       "4                                     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged = pheno_latest.merge(diag_keyed, on=\"_EID\", how=\"inner\", suffixes=(\"_pheno\",\"_dx\"))\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "\n",
    "# Quick peek\n",
    "display(merged[[\"_EID\",\"Sex\",\"Age\"] + [c for c in merged.columns if \"DX_\" in c][:6]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56e9adb2-006e-4685-8583-ba63130c2dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saved_at': '2025-11-06 15:11:08',\n",
       " 'pheno_sources': [{'file': 'HBN_R1_1_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R1_1_Pheno.csv',\n",
       "   'release_rank': 1.1},\n",
       "  {'file': 'HBN_R2_1_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R2_1_Pheno.csv',\n",
       "   'release_rank': 2.1},\n",
       "  {'file': 'HBN_R3_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R3_Pheno.csv',\n",
       "   'release_rank': 3.0},\n",
       "  {'file': 'HBN_R4_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R4_Pheno.csv',\n",
       "   'release_rank': 4.0},\n",
       "  {'file': 'HBN_R5_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R5_Pheno.csv',\n",
       "   'release_rank': 5.0},\n",
       "  {'file': 'HBN_R6_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R6_Pheno.csv',\n",
       "   'release_rank': 6.0},\n",
       "  {'file': 'HBN_R7_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R7_Pheno.csv',\n",
       "   'release_rank': 7.0},\n",
       "  {'file': 'HBN_R8_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R8_Pheno.csv',\n",
       "   'release_rank': 8.0},\n",
       "  {'file': 'HBN_R9_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R9_Pheno.csv',\n",
       "   'release_rank': 9.0},\n",
       "  {'file': 'HBN_R10_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R10_Pheno.csv',\n",
       "   'release_rank': 10.0},\n",
       "  {'file': 'HBN_R11_Pheno.csv',\n",
       "   'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/HBN_R11_Pheno.csv',\n",
       "   'release_rank': 11.0}],\n",
       " 'diagnosis_source': {'file': 'Diagnosis_ClinicianConsensus.csv',\n",
       "  'url': 'http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/Diagnosis_ClinicianConsensus.csv'},\n",
       " 'outputs': {'pheno_all_concat': 'data/raw/HBN_pheno_all_concat.csv',\n",
       "  'pheno_latest': 'data/raw/HBN_pheno_latest.csv',\n",
       "  'merged': 'data/interim/HBN_pheno_latest__with_diagnosis.csv'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_path = INTERIM/\"HBN_pheno_latest__with_diagnosis.csv\"\n",
    "merged.to_csv(merged_path, index=False)\n",
    "\n",
    "manifest = {\n",
    "    \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"pheno_sources\": [\n",
    "        {\"file\": f, \"url\": BASE+f, \"release_rank\": release_rank_from_filename(f)} for f in PHENO_FILES\n",
    "    ],\n",
    "    \"diagnosis_source\": {\"file\": DIAG_FILE, \"url\": BASE+DIAG_FILE},\n",
    "    \"outputs\": {\n",
    "        \"pheno_all_concat\": str(RAW/\"HBN_pheno_all_concat.csv\"),\n",
    "        \"pheno_latest\": str(RAW/\"HBN_pheno_latest.csv\"),\n",
    "        \"merged\": str(merged_path),\n",
    "    }\n",
    "}\n",
    "with open(RAW/\"MANIFEST.json\",\"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227166d-5c29-46af-a9a7-15df0cf16446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
