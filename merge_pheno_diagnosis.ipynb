{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb883ca-952c-4a8f-95ab-0e06cba854ab",
   "metadata": {},
   "source": [
    "# Setup: Import Libraries and Define Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e65efe-8574-49ff-9c46-f0715d7e9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define folder structure for organizing data\n",
    "# DATA folder contains all our data files\n",
    "DATA = Path(\"data\")\n",
    "RAW = DATA / \"raw\"              # Raw downloaded files\n",
    "INTERIM = DATA / \"interim\"      # Processed/merged files\n",
    "\n",
    "# Create folders if they don't exist\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base URL for downloading pheno files (using HTTP to avoid certificate issues)\n",
    "BASE = \"http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/_pheno/\"\n",
    "\n",
    "# List of all phenotype files we want to download (Releases 1-11)\n",
    "PHENO_FILES = [\n",
    "    \"HBN_R1_1_Pheno.csv\",\n",
    "    \"HBN_R2_1_Pheno.csv\",\n",
    "    \"HBN_R3_Pheno.csv\",\n",
    "    \"HBN_R4_Pheno.csv\",\n",
    "    \"HBN_R5_Pheno.csv\",\n",
    "    \"HBN_R6_Pheno.csv\",\n",
    "    \"HBN_R7_Pheno.csv\",\n",
    "    \"HBN_R8_Pheno.csv\",\n",
    "    \"HBN_R9_Pheno.csv\",\n",
    "    \"HBN_R10_Pheno.csv\",\n",
    "    \"HBN_R11_Pheno.csv\"\n",
    "]\n",
    "\n",
    "# Local diagnosis file (no longer downloading from API)\n",
    "DIAG_FILE = \"Diagnosis_ClinicianConsensus.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a0ae4-65a9-496f-b007-4d83792c0984",
   "metadata": {},
   "source": [
    "# Helper Functions for Data Download and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c535eba7-66ab-4e00-9d7f-69f32cb33b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_text(url, timeout=60):\n",
    "    \"\"\"\n",
    "    Download text content from a URL.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: The web address to download from\n",
    "    - timeout: How long to wait before giving up (seconds)\n",
    "    \n",
    "    Returns: The text content from the URL\n",
    "    \"\"\"\n",
    "    # Force HTTP instead of HTTPS for this specific server\n",
    "    if url.startswith(\"https://fcon_1000.projects.nitrc.org\"):\n",
    "        url = url.replace(\"https://\", \"http://\", 1)\n",
    "    \n",
    "    # Download the file\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    response.raise_for_status()  # Raise error if download failed\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def read_table_smart(url):\n",
    "    \"\"\"\n",
    "    Download a CSV/TSV file and automatically detect the separator.\n",
    "    \n",
    "    Figures out whether the file uses commas, tabs, semicolons, etc. to separate values.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: The web address of the file\n",
    "    \n",
    "    Returns: A pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Download the text content\n",
    "    text = http_text(url)\n",
    "    \n",
    "    # Look at the first 5000 characters to guess the separator\n",
    "    sample = text[:5000]\n",
    "    \n",
    "    try:\n",
    "        # Try to automatically detect the delimiter\n",
    "        sniffer = csv.Sniffer()\n",
    "        dialect = sniffer.sniff(sample, delimiters=[\",\", \";\", \"\\\\t\", \"|\"])\n",
    "        separator = dialect.delimiter\n",
    "    except Exception:\n",
    "        # If automatic detection fails, use the most common delimiter\n",
    "        separators = [\",\", \";\", \"\\\\t\", \"|\"]\n",
    "        separator = max(separators, key=sample.count)\n",
    "    \n",
    "    # Read the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(io.StringIO(text), sep=separator, engine=\"python\")\n",
    "    \n",
    "    # Clean up column names (remove extra spaces)\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_raw(dataframe, filename):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to the raw data folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: The pandas DataFrame to save\n",
    "    - filename: Name of the file (e.g., \"data.csv\")\n",
    "    \n",
    "    Returns: The full path where the file was saved\n",
    "    \"\"\"\n",
    "    file_path = RAW / filename\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def normalize_eid(value):\n",
    "    \"\"\"\n",
    "    Clean and standardize participant EID (participant ID).\n",
    "    \n",
    "    Examples:\n",
    "    - \"NDAR AA075 AMK\" → \"NDARAA075AMK\"\n",
    "    - \"ndar-aa112-dmh\" → \"NDARAA112DMH\"\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The raw EID value\n",
    "    \n",
    "    Returns: Cleaned EID or NaN if invalid\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string and uppercase\n",
    "    cleaned = str(value).strip().upper()\n",
    "    \n",
    "    # Remove all non-alphanumeric characters (spaces, dashes, etc.)\n",
    "    cleaned = re.sub(r\"[^A-Z0-9]\", \"\", cleaned)\n",
    "    \n",
    "    # Return NaN if empty after cleaning\n",
    "    return cleaned if cleaned else np.nan\n",
    "\n",
    "\n",
    "def get_release_number(filename):\n",
    "    \"\"\"\n",
    "    Extract the release version number from a filename.\n",
    "    \n",
    "    Examples:\n",
    "    - \"HBN_R1_1_Pheno.csv\" → 1.1\n",
    "    - \"HBN_R10_Pheno.csv\" → 10.0\n",
    "    - \"HBN_R11_Pheno.csv\" → 11.0\n",
    "    \n",
    "    This helps us track which release is newer when merging data.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Name of the pheno file\n",
    "    \n",
    "    Returns: Release number as a float\n",
    "    \"\"\"\n",
    "    # Look for pattern like \"_R10_\" or \"_R1_1_\"\n",
    "    match = re.search(r\"_R(\\\\d+)(?:_(\\\\d+))?_Pheno\\\\.csv$\", filename)\n",
    "    \n",
    "    if not match:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract major version (e.g., 10)\n",
    "    major = int(match.group(1))\n",
    "    \n",
    "    # Extract minor version if it exists (e.g., 1 in R1_1)\n",
    "    minor = int(match.group(2)) if match.group(2) else 0\n",
    "    \n",
    "    # Combine into single number (e.g., 10.0 or 1.1)\n",
    "    return float(f\"{major}.{minor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e35ea-9753-4fcd-85cc-f2e14c69ee86",
   "metadata": {},
   "source": [
    "# Download and Process All Phenotype Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29bf309-111f-4351-ae8c-c0c5153d99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HBN_R1_1_Pheno.csv...\n",
      "  ✓ Loaded: 797 rows, 9 columns\n",
      "Downloading HBN_R2_1_Pheno.csv...\n",
      "  ✓ Loaded: 256 rows, 9 columns\n",
      "Downloading HBN_R3_Pheno.csv...\n",
      "  ✓ Loaded: 317 rows, 9 columns\n",
      "Downloading HBN_R4_Pheno.csv...\n",
      "  ✓ Loaded: 558 rows, 9 columns\n",
      "Downloading HBN_R5_Pheno.csv...\n",
      "  ✓ Loaded: 391 rows, 9 columns\n",
      "Downloading HBN_R6_Pheno.csv...\n",
      "  ✓ Loaded: 336 rows, 9 columns\n",
      "Downloading HBN_R7_Pheno.csv...\n",
      "  ✓ Loaded: 692 rows, 9 columns\n",
      "Downloading HBN_R8_Pheno.csv...\n",
      "  ✓ Loaded: 470 rows, 9 columns\n",
      "Downloading HBN_R9_Pheno.csv...\n",
      "  ✓ Loaded: 422 rows, 9 columns\n",
      "Downloading HBN_R10_Pheno.csv...\n",
      "  ✓ Loaded: 847 rows, 9 columns\n",
      "Downloading HBN_R11_Pheno.csv...\n",
      "  ✓ Loaded: 1160 rows, 9 columns\n",
      "\\n============================================================\n",
      "Combined all releases: 6246 rows, 10 columns\n",
      "Latest data (one row per participant): 3432 participants\n",
      "\\n✓ Phenotype data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# This list will store all the downloaded phenotype data\n",
    "pheno_frames = []\n",
    "\n",
    "# Loop through each phenotype file and download it\n",
    "for filename in PHENO_FILES:\n",
    "    # Build the full URL\n",
    "    url = BASE + filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        \n",
    "        # Download and read the file\n",
    "        df = read_table_smart(url)\n",
    "        \n",
    "        # Save a copy to our raw data folder\n",
    "        save_raw(df, filename)\n",
    "        \n",
    "        # Add tracking columns to remember which release this came from\n",
    "        df[\"_release_file\"] = filename\n",
    "        df[\"_release_rank\"] = get_release_number(filename)\n",
    "        \n",
    "        # Normalize the participant ID (EID)\n",
    "        if \"EID\" in df.columns:\n",
    "            # If there's an explicit EID column, use it\n",
    "            df[\"_EID\"] = df[\"EID\"].apply(normalize_eid)\n",
    "        else:\n",
    "            # Otherwise, try to find an ID-like column\n",
    "            id_column = None\n",
    "            for col in df.columns:\n",
    "                # Look for columns named like \"eid\" or \"participant_eid\"\n",
    "                if re.fullmatch(r\"(participant_)?eid\", col, flags=re.I):\n",
    "                    id_column = col\n",
    "                    break\n",
    "            \n",
    "            if id_column:\n",
    "                df[\"_EID\"] = df[id_column].apply(normalize_eid)\n",
    "            else:\n",
    "                df[\"_EID\"] = np.nan\n",
    "        \n",
    "        # Add this DataFrame to our collection\n",
    "        pheno_frames.append(df)\n",
    "        print(f\"  ✓ Loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ WARNING: Failed to load {filename}: {e}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "\n",
    "# Combine all releases into one big DataFrame\n",
    "pheno_all = pd.concat(pheno_frames, ignore_index=True)\n",
    "print(f\"Combined all releases: {pheno_all.shape[0]} rows, {pheno_all.shape[1]} columns\")\n",
    "\n",
    "# Remove rows without a valid EID\n",
    "pheno_all = pheno_all[pheno_all[\"_EID\"].notna()]\n",
    "\n",
    "# Keep only the LATEST row for each participant\n",
    "# (If a participant appears in multiple releases, keep their newest data)\n",
    "pheno_all_sorted = pheno_all.sort_values([\"_EID\", \"_release_rank\"])\n",
    "pheno_latest = pheno_all_sorted.drop_duplicates(\"_EID\", keep=\"last\")\n",
    "\n",
    "print(f\"Latest data (one row per participant): {pheno_latest.shape[0]} participants\")\n",
    "\n",
    "# Save both versions\n",
    "save_raw(pheno_all, \"HBN_pheno_all_concat.csv\")\n",
    "save_raw(pheno_latest.drop(columns=[\"_release_file\", \"_release_rank\"]), \n",
    "         \"HBN_pheno_latest.csv\")\n",
    "\n",
    "print(\"\\\\n✓ Phenotype data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588bab8-dca0-4fc4-a347-8d276124ec85",
   "metadata": {},
   "source": [
    "# Load Diagnosis File (LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36aaaa50-1d77-4b96-ae4c-526fab1b5d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded diagnosis file: 4766 rows, 164 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/vsf2fnhd3bz458z2nvlnyrmm0000gn/T/ipykernel_55909/2516890633.py:2: DtypeWarning: Columns (150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  diag = pd.read_csv(diag_path)\n"
     ]
    }
   ],
   "source": [
    "diag_path = RAW / DIAG_FILE\n",
    "diag = pd.read_csv(diag_path)\n",
    "print(f\"✓ Loaded diagnosis file: {diag.shape[0]} rows, {diag.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cce13df-6ecc-4ddc-9828-24590a393971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3432 known participant IDs from phenotype data\n",
      "✓ Successfully matched 3373 participants\n",
      "  (out of 4766 total diagnosis records)\n"
     ]
    }
   ],
   "source": [
    "# Create a set of known participant IDs for matching\n",
    "known_eids = set(pheno_latest[\"_EID\"].dropna().unique())\n",
    "print(f\"We have {len(known_eids)} known participant IDs from phenotype data\")\n",
    "\n",
    "def extract_eid_from_identifiers(identifier_value, known_ids):\n",
    "    \"\"\"\n",
    "    Try to find a valid participant ID from the Identifiers field.\n",
    "    \n",
    "    The Identifiers field might contain multiple IDs or formatted text,\n",
    "    so we need to parse it carefully.\n",
    "    \n",
    "    Parameters:\n",
    "    - identifier_value: The raw value from the Identifiers column\n",
    "    - known_ids: Set of valid participant IDs we already know about\n",
    "    \n",
    "    Returns: A clean participant ID or NaN if not found\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(identifier_value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to uppercase string\n",
    "    text = str(identifier_value).upper()\n",
    "    \n",
    "    # Split on common separators (semicolons, commas, pipes, spaces)\n",
    "    tokens = re.split(r\"[;,|\\\\s]+\", text)\n",
    "    \n",
    "    # Strategy 1: Look for exact matches to known IDs\n",
    "    for token in tokens:\n",
    "        cleaned_token = normalize_eid(token)\n",
    "        if cleaned_token in known_ids:\n",
    "            return cleaned_token\n",
    "    \n",
    "    # Strategy 2: Look for HBN-style IDs using pattern matching\n",
    "    match = re.search(r\"\\\\bHBN[A-Z0-9]+\\\\b\", text)\n",
    "    if match:\n",
    "        cleaned_token = normalize_eid(match.group(0))\n",
    "        if cleaned_token in known_ids:\n",
    "            return cleaned_token\n",
    "    \n",
    "    # If can't find a valid ID, return NaN\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# Extract EIDs from the diagnosis data\n",
    "diag = diag.copy()  # Make a copy to avoid warnings\n",
    "diag[\"_EID\"] = diag[\"Identifiers\"].apply(\n",
    "    lambda val: extract_eid_from_identifiers(val, known_eids)\n",
    ")\n",
    "\n",
    "# Keep only rows where we successfully extracted an EID\n",
    "diag_with_eid = diag[diag[\"_EID\"].notna()]\n",
    "\n",
    "# Remove duplicate rows (keep first occurrence)\n",
    "diag_keyed = diag_with_eid.drop_duplicates(\"_EID\")\n",
    "\n",
    "print(f\"✓ Successfully matched {diag_keyed.shape[0]} participants\")\n",
    "print(f\"  (out of {diag.shape[0]} total diagnosis records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d635c-ec04-4798-a613-3f515bf7cd67",
   "metadata": {},
   "source": [
    "# Merge Phenotype and Diagnosis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef85b07e-d4e6-409e-9aa7-b7eca7b0f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Merged dataset: 3373 participants, 174 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c118f_row0_col7, #T_c118f_row0_col10, #T_c118f_row0_col11, #T_c118f_row1_col7, #T_c118f_row1_col10, #T_c118f_row1_col11, #T_c118f_row2_col7, #T_c118f_row2_col10, #T_c118f_row2_col11, #T_c118f_row3_col6, #T_c118f_row3_col8, #T_c118f_row3_col9, #T_c118f_row3_col10, #T_c118f_row3_col13, #T_c118f_row4_col8, #T_c118f_row4_col9, #T_c118f_row4_col10, #T_c118f_row4_col13, #T_c118f_row5_col7, #T_c118f_row5_col10, #T_c118f_row5_col11, #T_c118f_row6_col8, #T_c118f_row6_col9, #T_c118f_row6_col10, #T_c118f_row6_col13, #T_c118f_row7_col7, #T_c118f_row7_col10, #T_c118f_row7_col11, #T_c118f_row8_col8, #T_c118f_row8_col9, #T_c118f_row8_col10, #T_c118f_row8_col13, #T_c118f_row9_col8, #T_c118f_row9_col9, #T_c118f_row9_col10, #T_c118f_row9_col13, #T_c118f_row10_col8, #T_c118f_row10_col9, #T_c118f_row10_col10, #T_c118f_row10_col13 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c118f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c118f_level0_col0\" class=\"col_heading level0 col0\" >_EID</th>\n",
       "      <th id=\"T_c118f_level0_col1\" class=\"col_heading level0 col1\" >Sex</th>\n",
       "      <th id=\"T_c118f_level0_col2\" class=\"col_heading level0 col2\" >Age</th>\n",
       "      <th id=\"T_c118f_level0_col3\" class=\"col_heading level0 col3\" >Diagnosis_ClinicianConsensus,DX_01</th>\n",
       "      <th id=\"T_c118f_level0_col4\" class=\"col_heading level0 col4\" >Diagnosis_ClinicianConsensus,DX_01_ByHx</th>\n",
       "      <th id=\"T_c118f_level0_col5\" class=\"col_heading level0 col5\" >Diagnosis_ClinicianConsensus,DX_01_Cat</th>\n",
       "      <th id=\"T_c118f_level0_col6\" class=\"col_heading level0 col6\" >Diagnosis_ClinicianConsensus,DX_01_Code</th>\n",
       "      <th id=\"T_c118f_level0_col7\" class=\"col_heading level0 col7\" >Diagnosis_ClinicianConsensus,DX_01_Confirmed</th>\n",
       "      <th id=\"T_c118f_level0_col8\" class=\"col_heading level0 col8\" >Diagnosis_ClinicianConsensus,DX_01_New</th>\n",
       "      <th id=\"T_c118f_level0_col9\" class=\"col_heading level0 col9\" >Diagnosis_ClinicianConsensus,DX_01_PRem</th>\n",
       "      <th id=\"T_c118f_level0_col10\" class=\"col_heading level0 col10\" >Diagnosis_ClinicianConsensus,DX_01_Past_Doc</th>\n",
       "      <th id=\"T_c118f_level0_col11\" class=\"col_heading level0 col11\" >Diagnosis_ClinicianConsensus,DX_01_Presum</th>\n",
       "      <th id=\"T_c118f_level0_col12\" class=\"col_heading level0 col12\" >Diagnosis_ClinicianConsensus,DX_01_RC</th>\n",
       "      <th id=\"T_c118f_level0_col13\" class=\"col_heading level0 col13\" >Diagnosis_ClinicianConsensus,DX_01_Rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c118f_row0_col0\" class=\"data row0 col0\" >NDARAA075AMK</td>\n",
       "      <td id=\"T_c118f_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row0_col2\" class=\"data row0 col2\" >6.728040</td>\n",
       "      <td id=\"T_c118f_row0_col3\" class=\"data row0 col3\" >No Diagnosis Given</td>\n",
       "      <td id=\"T_c118f_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row0_col5\" class=\"data row0 col5\" >No Diagnosis Given</td>\n",
       "      <td id=\"T_c118f_row0_col6\" class=\"data row0 col6\" >No Diagnosis Given</td>\n",
       "      <td id=\"T_c118f_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
       "      <td id=\"T_c118f_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_c118f_row0_col12\" class=\"data row0 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row0_col13\" class=\"data row0 col13\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c118f_row1_col0\" class=\"data row1 col0\" >NDARAA112DMH</td>\n",
       "      <td id=\"T_c118f_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row1_col2\" class=\"data row1 col2\" >5.545744</td>\n",
       "      <td id=\"T_c118f_row1_col3\" class=\"data row1 col3\" >ADHD-Combined Type</td>\n",
       "      <td id=\"T_c118f_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row1_col5\" class=\"data row1 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row1_col6\" class=\"data row1 col6\" >F90.2</td>\n",
       "      <td id=\"T_c118f_row1_col7\" class=\"data row1 col7\" >nan</td>\n",
       "      <td id=\"T_c118f_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_c118f_row1_col12\" class=\"data row1 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row1_col13\" class=\"data row1 col13\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c118f_row2_col0\" class=\"data row2 col0\" >NDARAA117NEJ</td>\n",
       "      <td id=\"T_c118f_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row2_col2\" class=\"data row2 col2\" >7.475929</td>\n",
       "      <td id=\"T_c118f_row2_col3\" class=\"data row2 col3\" >ADHD-Combined Type</td>\n",
       "      <td id=\"T_c118f_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row2_col5\" class=\"data row2 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row2_col6\" class=\"data row2 col6\" >F90.2</td>\n",
       "      <td id=\"T_c118f_row2_col7\" class=\"data row2 col7\" >nan</td>\n",
       "      <td id=\"T_c118f_row2_col8\" class=\"data row2 col8\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row2_col11\" class=\"data row2 col11\" >nan</td>\n",
       "      <td id=\"T_c118f_row2_col12\" class=\"data row2 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row2_col13\" class=\"data row2 col13\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c118f_row3_col0\" class=\"data row3 col0\" >NDARAA306NT2</td>\n",
       "      <td id=\"T_c118f_row3_col1\" class=\"data row3 col1\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row3_col2\" class=\"data row3 col2\" >21.216746</td>\n",
       "      <td id=\"T_c118f_row3_col3\" class=\"data row3 col3\" >Generalized Anxiety Disorder</td>\n",
       "      <td id=\"T_c118f_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row3_col5\" class=\"data row3 col5\" >Anxiety Disorders</td>\n",
       "      <td id=\"T_c118f_row3_col6\" class=\"data row3 col6\" >nan</td>\n",
       "      <td id=\"T_c118f_row3_col7\" class=\"data row3 col7\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row3_col11\" class=\"data row3 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row3_col12\" class=\"data row3 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row3_col13\" class=\"data row3 col13\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c118f_row4_col0\" class=\"data row4 col0\" >NDARAA504CRN</td>\n",
       "      <td id=\"T_c118f_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row4_col2\" class=\"data row4 col2\" >9.165297</td>\n",
       "      <td id=\"T_c118f_row4_col3\" class=\"data row4 col3\" >ADHD-Inattentive Type</td>\n",
       "      <td id=\"T_c118f_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row4_col5\" class=\"data row4 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row4_col6\" class=\"data row4 col6\" >F90.0</td>\n",
       "      <td id=\"T_c118f_row4_col7\" class=\"data row4 col7\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row4_col11\" class=\"data row4 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row4_col12\" class=\"data row4 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row4_col13\" class=\"data row4 col13\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c118f_row5_col0\" class=\"data row5 col0\" >NDARAA536PTU</td>\n",
       "      <td id=\"T_c118f_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row5_col2\" class=\"data row5 col2\" >11.998402</td>\n",
       "      <td id=\"T_c118f_row5_col3\" class=\"data row5 col3\" >ADHD-Inattentive Type</td>\n",
       "      <td id=\"T_c118f_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row5_col5\" class=\"data row5 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row5_col6\" class=\"data row5 col6\" >F90.0</td>\n",
       "      <td id=\"T_c118f_row5_col7\" class=\"data row5 col7\" >nan</td>\n",
       "      <td id=\"T_c118f_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row5_col9\" class=\"data row5 col9\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row5_col10\" class=\"data row5 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_c118f_row5_col12\" class=\"data row5 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row5_col13\" class=\"data row5 col13\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c118f_row6_col0\" class=\"data row6 col0\" >NDARAA947ZG5</td>\n",
       "      <td id=\"T_c118f_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row6_col2\" class=\"data row6 col2\" >13.627880</td>\n",
       "      <td id=\"T_c118f_row6_col3\" class=\"data row6 col3\" >ADHD-Combined Type</td>\n",
       "      <td id=\"T_c118f_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row6_col5\" class=\"data row6 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row6_col6\" class=\"data row6 col6\" >F90.2</td>\n",
       "      <td id=\"T_c118f_row6_col7\" class=\"data row6 col7\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row6_col8\" class=\"data row6 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row6_col9\" class=\"data row6 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row6_col10\" class=\"data row6 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row6_col11\" class=\"data row6 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row6_col12\" class=\"data row6 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row6_col13\" class=\"data row6 col13\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c118f_row7_col0\" class=\"data row7 col0\" >NDARAA948VFH</td>\n",
       "      <td id=\"T_c118f_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row7_col2\" class=\"data row7 col2\" >7.982660</td>\n",
       "      <td id=\"T_c118f_row7_col3\" class=\"data row7 col3\" >ADHD-Combined Type</td>\n",
       "      <td id=\"T_c118f_row7_col4\" class=\"data row7 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row7_col5\" class=\"data row7 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row7_col6\" class=\"data row7 col6\" >F90.2</td>\n",
       "      <td id=\"T_c118f_row7_col7\" class=\"data row7 col7\" >nan</td>\n",
       "      <td id=\"T_c118f_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row7_col9\" class=\"data row7 col9\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row7_col10\" class=\"data row7 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row7_col11\" class=\"data row7 col11\" >nan</td>\n",
       "      <td id=\"T_c118f_row7_col12\" class=\"data row7 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row7_col13\" class=\"data row7 col13\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c118f_row8_col0\" class=\"data row8 col0\" >NDARAB055BPR</td>\n",
       "      <td id=\"T_c118f_row8_col1\" class=\"data row8 col1\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row8_col2\" class=\"data row8 col2\" >10.793862</td>\n",
       "      <td id=\"T_c118f_row8_col3\" class=\"data row8 col3\" >ADHD-Combined Type</td>\n",
       "      <td id=\"T_c118f_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row8_col5\" class=\"data row8 col5\" >Neurodevelopmental Disorders</td>\n",
       "      <td id=\"T_c118f_row8_col6\" class=\"data row8 col6\" >F90.2</td>\n",
       "      <td id=\"T_c118f_row8_col7\" class=\"data row8 col7\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row8_col8\" class=\"data row8 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row8_col9\" class=\"data row8 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row8_col10\" class=\"data row8 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row8_col11\" class=\"data row8 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row8_col12\" class=\"data row8 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row8_col13\" class=\"data row8 col13\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c118f_row9_col0\" class=\"data row9 col0\" >NDARAB348EWR</td>\n",
       "      <td id=\"T_c118f_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row9_col2\" class=\"data row9 col2\" >5.805384</td>\n",
       "      <td id=\"T_c118f_row9_col3\" class=\"data row9 col3\" >Other Specified Depressive Disorder</td>\n",
       "      <td id=\"T_c118f_row9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row9_col5\" class=\"data row9 col5\" >Depressive Disorders</td>\n",
       "      <td id=\"T_c118f_row9_col6\" class=\"data row9 col6\" >F32.8</td>\n",
       "      <td id=\"T_c118f_row9_col7\" class=\"data row9 col7\" >1.000000</td>\n",
       "      <td id=\"T_c118f_row9_col8\" class=\"data row9 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row9_col9\" class=\"data row9 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row9_col10\" class=\"data row9 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row9_col11\" class=\"data row9 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row9_col12\" class=\"data row9 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row9_col13\" class=\"data row9 col13\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c118f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c118f_row10_col0\" class=\"data row10 col0\" >NDARAB375XZ1</td>\n",
       "      <td id=\"T_c118f_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row10_col2\" class=\"data row10 col2\" >11.569130</td>\n",
       "      <td id=\"T_c118f_row10_col3\" class=\"data row10 col3\" >No Diagnosis Given: Incomplete Eval</td>\n",
       "      <td id=\"T_c118f_row10_col4\" class=\"data row10 col4\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row10_col5\" class=\"data row10 col5\" >No Diagnosis Given: Incomplete Eval</td>\n",
       "      <td id=\"T_c118f_row10_col6\" class=\"data row10 col6\" >No Diagnosis Given: Incomplete Eval</td>\n",
       "      <td id=\"T_c118f_row10_col7\" class=\"data row10 col7\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row10_col8\" class=\"data row10 col8\" >nan</td>\n",
       "      <td id=\"T_c118f_row10_col9\" class=\"data row10 col9\" >nan</td>\n",
       "      <td id=\"T_c118f_row10_col10\" class=\"data row10 col10\" >nan</td>\n",
       "      <td id=\"T_c118f_row10_col11\" class=\"data row10 col11\" >0.000000</td>\n",
       "      <td id=\"T_c118f_row10_col12\" class=\"data row10 col12\" >0</td>\n",
       "      <td id=\"T_c118f_row10_col13\" class=\"data row10 col13\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1597cba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge the two datasets on participant ID (_EID)\n",
    "# Use an \"inner\" join, which keeps only participants present in BOTH datasets\n",
    "merged = pheno_latest.merge(\n",
    "    diag_keyed,\n",
    "    on=\"_EID\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_pheno\", \"_dx\")  # Add suffixes if column names overlap\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged dataset: {merged.shape[0]} participants, {merged.shape[1]} columns\")\n",
    "\n",
    "# Preview the merged data\n",
    "base_cols = [\"_EID\", \"Sex\", \"Age\"]\n",
    "dx_cols = [c for c in merged.columns if \"DX_\" in c][:11]\n",
    "preview_cols = [c for c in (base_cols + dx_cols) if c in merged.columns]\n",
    "df_prev = merged.loc[:, preview_cols].head(11).copy()\n",
    "\n",
    "if \"Age\" in df_prev.columns:\n",
    "    df_prev[\"Age\"] = pd.to_numeric(df_prev[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "display(df_prev.style.highlight_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85291-1326-40d4-91ed-658e775c7297",
   "metadata": {},
   "source": [
    "# Save Final Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca6400f2-ee75-4720-b8ec-5318ffc40f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved merged dataset to: data/interim/HBN_pheno_with_diagnosis.csv\n",
      "✓ Saved processing manifest to: data/raw/MANIFEST.json\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "  Total Pheno Records: 4,239\n",
      "  Unique Participants: 3,432\n",
      "  Diagnosis Records: 4,766\n",
      "  Matched Participants: 3,373\n",
      "  Final Merged: 3,373\n"
     ]
    }
   ],
   "source": [
    "merged_path = INTERIM / \"HBN_pheno_with_diagnosis.csv\"\n",
    "merged.to_csv(merged_path, index=False)\n",
    "print(f\"\\n✓ Saved merged dataset to: {merged_path}\")\n",
    "\n",
    "# Create a manifest file documenting what we did\n",
    "manifest = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"description\": \"HBN phenotype data merged with clinical diagnosis\",\n",
    "    \"pheno_sources\": [\n",
    "        {\n",
    "            \"file\": filename,\n",
    "            \"url\": BASE + filename,\n",
    "            \"release_version\": get_release_number(filename)\n",
    "        }\n",
    "        for filename in PHENO_FILES\n",
    "    ],\n",
    "    \"diagnosis_source\": {\n",
    "        \"file\": DIAG_FILE,\n",
    "        \"source\": \"local file (data/raw/)\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"pheno_all_concat\": str(RAW / \"HBN_pheno_all_concat.csv\"),\n",
    "        \"pheno_latest\": str(RAW / \"HBN_pheno_latest.csv\"),\n",
    "        \"merged_dataset\": str(merged_path)\n",
    "    },\n",
    "    \"record_counts\": {\n",
    "        \"total_pheno_records\": int(pheno_all.shape[0]),\n",
    "        \"unique_participants\": int(pheno_latest.shape[0]),\n",
    "        \"diagnosis_records\": int(diag.shape[0]),\n",
    "        \"matched_participants\": int(diag_keyed.shape[0]),\n",
    "        \"final_merged\": int(merged.shape[0])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save manifest as JSON\n",
    "manifest_path = RAW / \"MANIFEST.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved processing manifest to: {manifest_path}\")\n",
    "\n",
    "# Display the manifest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in manifest[\"record_counts\"].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd273b-c9ea-4aa1-a985-414f54b2a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
